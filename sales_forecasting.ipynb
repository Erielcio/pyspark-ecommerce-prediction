{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SparkConf().setAll([\n",
    "    ('spark.executor.cores', '2'), ('spark.executor.memory', '4g'), ('spark.driver.memory','4g'), ('spark.submit.deployMode','client')\n",
    "])\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=config)\n",
    "\n",
    "#sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.getConf()#.get(\"spark.executor.instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397884"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = sqlContext.read.csv('gs://kaggle01_test/notebooks/jupyter/pyspark-ecommerce-prediction-main/ecommerce-data100.csv', header=True)\n",
    "#df = sqlContext.read.csv('ecommerce-data.csv', header=True)\n",
    "df = sqlContext.read.csv('user_ecommerce-data.csv', header=True, inferSchema=True)\n",
    "df = df.drop('CustomerID', 'UnitPrice')\n",
    "df = df.withColumn(\"Date\", to_date(col(\"Date\")))\n",
    "#df = df.withColumn(\"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"), 'mm-dd-yyyy HH:mm:ss'))\n",
    "\n",
    "#df.show()\n",
    "#df = df.drop_duplicates(['CustomerID'])\n",
    "#df = df.na.drop()\n",
    "# df.dropna(subset=['Description']).show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(df, column):\n",
    "    quantiles = df.stat.approxQuantile(column, [0.05, 0.95], 0.0)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lowerRange = Q1 - 1.5 * IQR\n",
    "    upperRange = Q3 + 1.5 * IQR\n",
    "    df.filter((df[column] < lowerRange) | (df[column] > upperRange)).show()\n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    quantiles = df.stat.approxQuantile(column, [0.05, 0.95], 0.0)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lowerRange = Q1 - 1.5 * IQR\n",
    "    upperRange = Q3 + 1.5 * IQR\n",
    "    df = df.filter((df[column] >= lowerRange) | (df[column] <= upperRange))\n",
    "\n",
    "remove_outliers(df, 'Quantity')\n",
    "remove_outliers(df, 'TotalPrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = df.select('Date', 'Quantity', 'TotalPrice').groupby('Date').sum()\n",
    "agg = agg.withColumnRenamed('sum(Quantity)', 'Quantity')\n",
    "agg = agg.withColumnRenamed('sum(TotalPrice)', 'TotalPrice')\n",
    "\n",
    "agg = agg.withColumn('Month', month('Date'))\n",
    "agg = agg.withColumn('DayOfMonth', dayofmonth('Date'))\n",
    "agg = agg.withColumn('DayOfWeek', dayofweek('Date'))\n",
    "agg = agg.withColumn('DayOfYear', dayofyear('Date'))\n",
    "agg = agg.withColumn('Weekend', dayofweek('Date').isin(6,7).cast('int'))\n",
    "\n",
    "#agg.show()\n",
    "#agg.write.csv('forecasting-agg.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "agg = agg.orderBy(asc(\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_agg = agg.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274.5, 30.5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_agg['Date'].count() * 0.9, pd_agg['Date'].count() * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "275+30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "agg = agg.orderBy(asc(\"Date\"))\n",
    "agg_train = agg.limit(275)\n",
    "agg_test = agg.orderBy(desc(\"Date\")).limit(30).orderBy(\"Date\")\n",
    "#train.show()\n",
    "#test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([datetime.date(2010, 12, 1), datetime.date(2010, 12, 2),\n",
       "        datetime.date(2010, 12, 3), datetime.date(2010, 12, 5),\n",
       "        datetime.date(2010, 12, 6), datetime.date(2010, 12, 7),\n",
       "        datetime.date(2010, 12, 8), datetime.date(2010, 12, 9),\n",
       "        datetime.date(2010, 12, 10), datetime.date(2010, 12, 12),\n",
       "        datetime.date(2010, 12, 13), datetime.date(2010, 12, 14),\n",
       "        datetime.date(2010, 12, 15), datetime.date(2010, 12, 16),\n",
       "        datetime.date(2010, 12, 17), datetime.date(2010, 12, 19),\n",
       "        datetime.date(2010, 12, 20), datetime.date(2010, 12, 21),\n",
       "        datetime.date(2010, 12, 22), datetime.date(2010, 12, 23),\n",
       "        datetime.date(2011, 1, 4), datetime.date(2011, 1, 5),\n",
       "        datetime.date(2011, 1, 6), datetime.date(2011, 1, 7),\n",
       "        datetime.date(2011, 1, 9), datetime.date(2011, 1, 10),\n",
       "        datetime.date(2011, 1, 11), datetime.date(2011, 1, 12),\n",
       "        datetime.date(2011, 1, 13), datetime.date(2011, 1, 14),\n",
       "        datetime.date(2011, 1, 16), datetime.date(2011, 1, 17),\n",
       "        datetime.date(2011, 1, 18), datetime.date(2011, 1, 19),\n",
       "        datetime.date(2011, 1, 20), datetime.date(2011, 1, 21),\n",
       "        datetime.date(2011, 1, 23), datetime.date(2011, 1, 24),\n",
       "        datetime.date(2011, 1, 25), datetime.date(2011, 1, 26),\n",
       "        datetime.date(2011, 1, 27), datetime.date(2011, 1, 28),\n",
       "        datetime.date(2011, 1, 30), datetime.date(2011, 1, 31),\n",
       "        datetime.date(2011, 2, 1), datetime.date(2011, 2, 2),\n",
       "        datetime.date(2011, 2, 3), datetime.date(2011, 2, 4),\n",
       "        datetime.date(2011, 2, 6), datetime.date(2011, 2, 7),\n",
       "        datetime.date(2011, 2, 8), datetime.date(2011, 2, 9),\n",
       "        datetime.date(2011, 2, 10), datetime.date(2011, 2, 11),\n",
       "        datetime.date(2011, 2, 13), datetime.date(2011, 2, 14),\n",
       "        datetime.date(2011, 2, 15), datetime.date(2011, 2, 16),\n",
       "        datetime.date(2011, 2, 17), datetime.date(2011, 2, 18),\n",
       "        datetime.date(2011, 2, 20), datetime.date(2011, 2, 21),\n",
       "        datetime.date(2011, 2, 22), datetime.date(2011, 2, 23),\n",
       "        datetime.date(2011, 2, 24), datetime.date(2011, 2, 25),\n",
       "        datetime.date(2011, 2, 27), datetime.date(2011, 2, 28),\n",
       "        datetime.date(2011, 3, 1), datetime.date(2011, 3, 2),\n",
       "        datetime.date(2011, 3, 3), datetime.date(2011, 3, 4),\n",
       "        datetime.date(2011, 3, 6), datetime.date(2011, 3, 7),\n",
       "        datetime.date(2011, 3, 8), datetime.date(2011, 3, 9),\n",
       "        datetime.date(2011, 3, 10), datetime.date(2011, 3, 11),\n",
       "        datetime.date(2011, 3, 13), datetime.date(2011, 3, 14),\n",
       "        datetime.date(2011, 3, 15), datetime.date(2011, 3, 16),\n",
       "        datetime.date(2011, 3, 17), datetime.date(2011, 3, 18),\n",
       "        datetime.date(2011, 3, 20), datetime.date(2011, 3, 21),\n",
       "        datetime.date(2011, 3, 22), datetime.date(2011, 3, 23),\n",
       "        datetime.date(2011, 3, 24), datetime.date(2011, 3, 25),\n",
       "        datetime.date(2011, 3, 27), datetime.date(2011, 3, 28),\n",
       "        datetime.date(2011, 3, 29), datetime.date(2011, 3, 30),\n",
       "        datetime.date(2011, 3, 31), datetime.date(2011, 4, 1),\n",
       "        datetime.date(2011, 4, 3), datetime.date(2011, 4, 4),\n",
       "        datetime.date(2011, 4, 5), datetime.date(2011, 4, 6),\n",
       "        datetime.date(2011, 4, 7), datetime.date(2011, 4, 8),\n",
       "        datetime.date(2011, 4, 10), datetime.date(2011, 4, 11),\n",
       "        datetime.date(2011, 4, 12), datetime.date(2011, 4, 13),\n",
       "        datetime.date(2011, 4, 14), datetime.date(2011, 4, 15),\n",
       "        datetime.date(2011, 4, 17), datetime.date(2011, 4, 18),\n",
       "        datetime.date(2011, 4, 19), datetime.date(2011, 4, 20),\n",
       "        datetime.date(2011, 4, 21), datetime.date(2011, 4, 26),\n",
       "        datetime.date(2011, 4, 27), datetime.date(2011, 4, 28),\n",
       "        datetime.date(2011, 5, 1), datetime.date(2011, 5, 3),\n",
       "        datetime.date(2011, 5, 4), datetime.date(2011, 5, 5),\n",
       "        datetime.date(2011, 5, 6), datetime.date(2011, 5, 8),\n",
       "        datetime.date(2011, 5, 9), datetime.date(2011, 5, 10),\n",
       "        datetime.date(2011, 5, 11), datetime.date(2011, 5, 12),\n",
       "        datetime.date(2011, 5, 13), datetime.date(2011, 5, 15),\n",
       "        datetime.date(2011, 5, 16), datetime.date(2011, 5, 17),\n",
       "        datetime.date(2011, 5, 18), datetime.date(2011, 5, 19),\n",
       "        datetime.date(2011, 5, 20), datetime.date(2011, 5, 22),\n",
       "        datetime.date(2011, 5, 23), datetime.date(2011, 5, 24),\n",
       "        datetime.date(2011, 5, 25), datetime.date(2011, 5, 26),\n",
       "        datetime.date(2011, 5, 27), datetime.date(2011, 5, 29),\n",
       "        datetime.date(2011, 5, 31), datetime.date(2011, 6, 1),\n",
       "        datetime.date(2011, 6, 2), datetime.date(2011, 6, 3),\n",
       "        datetime.date(2011, 6, 5), datetime.date(2011, 6, 6),\n",
       "        datetime.date(2011, 6, 7), datetime.date(2011, 6, 8),\n",
       "        datetime.date(2011, 6, 9), datetime.date(2011, 6, 10),\n",
       "        datetime.date(2011, 6, 12), datetime.date(2011, 6, 13),\n",
       "        datetime.date(2011, 6, 14), datetime.date(2011, 6, 15),\n",
       "        datetime.date(2011, 6, 16), datetime.date(2011, 6, 17),\n",
       "        datetime.date(2011, 6, 19), datetime.date(2011, 6, 20),\n",
       "        datetime.date(2011, 6, 21), datetime.date(2011, 6, 22),\n",
       "        datetime.date(2011, 6, 23), datetime.date(2011, 6, 24),\n",
       "        datetime.date(2011, 6, 26), datetime.date(2011, 6, 27),\n",
       "        datetime.date(2011, 6, 28), datetime.date(2011, 6, 29),\n",
       "        datetime.date(2011, 6, 30), datetime.date(2011, 7, 1),\n",
       "        datetime.date(2011, 7, 3), datetime.date(2011, 7, 4),\n",
       "        datetime.date(2011, 7, 5), datetime.date(2011, 7, 6),\n",
       "        datetime.date(2011, 7, 7), datetime.date(2011, 7, 8),\n",
       "        datetime.date(2011, 7, 10), datetime.date(2011, 7, 11),\n",
       "        datetime.date(2011, 7, 12), datetime.date(2011, 7, 13),\n",
       "        datetime.date(2011, 7, 14), datetime.date(2011, 7, 15),\n",
       "        datetime.date(2011, 7, 17), datetime.date(2011, 7, 18),\n",
       "        datetime.date(2011, 7, 19), datetime.date(2011, 7, 20),\n",
       "        datetime.date(2011, 7, 21), datetime.date(2011, 7, 22),\n",
       "        datetime.date(2011, 7, 24), datetime.date(2011, 7, 25),\n",
       "        datetime.date(2011, 7, 26), datetime.date(2011, 7, 27),\n",
       "        datetime.date(2011, 7, 28), datetime.date(2011, 7, 29),\n",
       "        datetime.date(2011, 7, 31), datetime.date(2011, 8, 1),\n",
       "        datetime.date(2011, 8, 2), datetime.date(2011, 8, 3),\n",
       "        datetime.date(2011, 8, 4), datetime.date(2011, 8, 5),\n",
       "        datetime.date(2011, 8, 7), datetime.date(2011, 8, 8),\n",
       "        datetime.date(2011, 8, 9), datetime.date(2011, 8, 10),\n",
       "        datetime.date(2011, 8, 11), datetime.date(2011, 8, 12),\n",
       "        datetime.date(2011, 8, 14), datetime.date(2011, 8, 15),\n",
       "        datetime.date(2011, 8, 16), datetime.date(2011, 8, 17),\n",
       "        datetime.date(2011, 8, 18), datetime.date(2011, 8, 19),\n",
       "        datetime.date(2011, 8, 21), datetime.date(2011, 8, 22),\n",
       "        datetime.date(2011, 8, 23), datetime.date(2011, 8, 24),\n",
       "        datetime.date(2011, 8, 25), datetime.date(2011, 8, 26),\n",
       "        datetime.date(2011, 8, 28), datetime.date(2011, 8, 30),\n",
       "        datetime.date(2011, 8, 31), datetime.date(2011, 9, 1),\n",
       "        datetime.date(2011, 9, 2), datetime.date(2011, 9, 4),\n",
       "        datetime.date(2011, 9, 5), datetime.date(2011, 9, 6),\n",
       "        datetime.date(2011, 9, 7), datetime.date(2011, 9, 8),\n",
       "        datetime.date(2011, 9, 9), datetime.date(2011, 9, 11),\n",
       "        datetime.date(2011, 9, 12), datetime.date(2011, 9, 13),\n",
       "        datetime.date(2011, 9, 14), datetime.date(2011, 9, 15),\n",
       "        datetime.date(2011, 9, 16), datetime.date(2011, 9, 18),\n",
       "        datetime.date(2011, 9, 19), datetime.date(2011, 9, 20),\n",
       "        datetime.date(2011, 9, 21), datetime.date(2011, 9, 22),\n",
       "        datetime.date(2011, 9, 23), datetime.date(2011, 9, 25),\n",
       "        datetime.date(2011, 9, 26), datetime.date(2011, 9, 27),\n",
       "        datetime.date(2011, 9, 28), datetime.date(2011, 9, 29)],\n",
       "       dtype=object),\n",
       " array([datetime.date(2011, 9, 30), datetime.date(2011, 10, 2),\n",
       "        datetime.date(2011, 10, 3), datetime.date(2011, 10, 4),\n",
       "        datetime.date(2011, 10, 5), datetime.date(2011, 10, 6),\n",
       "        datetime.date(2011, 10, 7), datetime.date(2011, 10, 9),\n",
       "        datetime.date(2011, 10, 10), datetime.date(2011, 10, 11),\n",
       "        datetime.date(2011, 10, 12), datetime.date(2011, 10, 13),\n",
       "        datetime.date(2011, 10, 14), datetime.date(2011, 10, 16),\n",
       "        datetime.date(2011, 10, 17), datetime.date(2011, 10, 18),\n",
       "        datetime.date(2011, 10, 19), datetime.date(2011, 10, 20),\n",
       "        datetime.date(2011, 10, 21), datetime.date(2011, 10, 23),\n",
       "        datetime.date(2011, 10, 24), datetime.date(2011, 10, 25),\n",
       "        datetime.date(2011, 10, 26), datetime.date(2011, 10, 27),\n",
       "        datetime.date(2011, 10, 28), datetime.date(2011, 10, 30),\n",
       "        datetime.date(2011, 10, 31), datetime.date(2011, 11, 1),\n",
       "        datetime.date(2011, 11, 2), datetime.date(2011, 11, 3),\n",
       "        datetime.date(2011, 11, 4), datetime.date(2011, 11, 6),\n",
       "        datetime.date(2011, 11, 7), datetime.date(2011, 11, 8),\n",
       "        datetime.date(2011, 11, 9), datetime.date(2011, 11, 10),\n",
       "        datetime.date(2011, 11, 11), datetime.date(2011, 11, 13),\n",
       "        datetime.date(2011, 11, 14), datetime.date(2011, 11, 15),\n",
       "        datetime.date(2011, 11, 16), datetime.date(2011, 11, 17),\n",
       "        datetime.date(2011, 11, 18), datetime.date(2011, 11, 20),\n",
       "        datetime.date(2011, 11, 21), datetime.date(2011, 11, 22),\n",
       "        datetime.date(2011, 11, 23), datetime.date(2011, 11, 24),\n",
       "        datetime.date(2011, 11, 25), datetime.date(2011, 11, 27),\n",
       "        datetime.date(2011, 11, 28), datetime.date(2011, 11, 29),\n",
       "        datetime.date(2011, 11, 30), datetime.date(2011, 12, 1),\n",
       "        datetime.date(2011, 12, 2), datetime.date(2011, 12, 4),\n",
       "        datetime.date(2011, 12, 5), datetime.date(2011, 12, 6),\n",
       "        datetime.date(2011, 12, 7), datetime.date(2011, 12, 8),\n",
       "        datetime.date(2011, 12, 9)], dtype=object))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train = agg_train.toPandas()\n",
    "pd_test = agg_test.toPandas()\n",
    "pd_train['Date'].unique(), pd_test['Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bucketizer = Bucketizer(splits=[ 0, 2, 5, 8, 11, 14, 15, 5000], inputCol=\"Quantity\", outputCol=\"QuantityRange\")\n",
    "df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "\n",
    "bucketizer = Bucketizer(splits=[ 0, 1, 2, 3, 4, 20], inputCol=\"UnitPrice\", outputCol=\"PriceRange\")\n",
    "df = bucketizer.setHandleInvalid(\"keep\").transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"Month\", outputCol=\"DateRange\")\n",
    "\n",
    "result = discretizer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "si = StringIndexer(inputCol='StockCode', outputCol='StockCodeIndex')\n",
    "df = si.fit(df).transform(df)\n",
    "\n",
    "si = StringIndexer(inputCol='Country', outputCol='CountryIndex')\n",
    "df = si.fit(df).transform(df)\n",
    "\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"CountryIndex\", df[\"CountryIndex\"].cast('int'))\n",
    "df = df.withColumn(\"StockCodeIndex\", df[\"StockCodeIndex\"].cast('int'))\n",
    "\n",
    "df = df.withColumn(\"QuantityRange\", df[\"QuantityRange\"].cast('int'))\n",
    "df = df.withColumn(\"PriceRange\", df[\"PriceRange\"].cast('int'))\n",
    "\n",
    "df = df.withColumn(\"Cluster\", df[\"Cluster\"].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agg = sqlContext.read.csv('forecasting-agg.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    #inputCols=['Quantity', 'UnitPrice', 'QuantityRange', 'PriceRange', 'Month', 'CountryIndex', 'StockCodeIndex', 'Cluster'],\n",
    "    inputCols=['Quantity','TotalPrice','Month','DayOfMonth','DayOfWeek','DayOfYear','Weekend'],\n",
    "    #inputCols=['Quantity','TotalPrice','Month','DayOfMonth','DayOfWeek'],\n",
    "    #inputCols=['Quantity','TotalPrice','Month','DayOfWeek'],\n",
    "    #inputCols=['Quantity','TotalPrice','Month'],\n",
    "    #inputCols=['Quantity','TotalPrice'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "train = assembler.transform(agg_train).select(['features', 'TotalPrice'])\n",
    "test = assembler.transform(agg_test).select(['features', 'TotalPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prediction):\n",
    "    evaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    print(f\"Root Mean Squared Error (RMSE) on test data = {evaluator.evaluate(prediction)}\")\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "    print(f\"Mean Squared Error (MSE) on test data = {evaluator.evaluate(prediction)}\")\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    print(f\"MAE on test data = {evaluator.evaluate(prediction)}\")\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    print(f\"R2 on test data = {evaluator.evaluate(prediction)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.0029825441442871987\n",
      "Mean Squared Error (MSE) on test data = 8.895569572621857e-06\n",
      "MAE on test data = 0.0015331794440664288\n",
      "R2 on test data = 0.9999999999999852\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#lr = LinearRegression(maxIter=5, regParam=0.3, elasticNetParam=0.8)\n",
    "lr = LinearRegression(labelCol='TotalPrice')\n",
    "lrparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.001, 0.01, 0.1, 0.5, 1.0, 2.0])\n",
    "             #.addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "             #.addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10, 20, 50])\n",
    "             #.addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "lrevaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "lrcv = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=lrparamGrid,\n",
    "                          evaluator=lrevaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "lrcvModel = lrcv.fit(train)\n",
    "\n",
    "#lrcvSummary = lrcvModel.bestModel.summary\n",
    "#print(\"Coefficient Standard Errors: \" + str(lrcvSummary.coefficientStandardErrors))\n",
    "#print(\"P Values: \" + str(lrcvSummary.pValues)) # Last element is the intercept\n",
    "\n",
    "lrpredictions = lrcvModel.transform(test)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluate(lrpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LinearRegression_0a7febd1d8cb', name='regParam', doc='regularization parameter (>= 0).'): 0.5,\n",
       " Param(parent='LinearRegression_0a7febd1d8cb', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0,\n",
       " Param(parent='LinearRegression_0a7febd1d8cb', name='maxIter', doc='max number of iterations (>= 0).'): 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcvModel.getEstimatorParamMaps()[ np.argmax(lrcvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.029824941532026635\n",
      "Mean Squared Error (MSE) on test data = 0.0008895271373888073\n",
      "MAE on test data = 0.015333228396560695\n",
      "R2 on test data = 0.9999999999985182\n"
     ]
    }
   ],
   "source": [
    "evaluate(lrpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 15660.475405175966\n",
      "Mean Squared Error (MSE) on test data = 245250489.91612136\n",
      "MAE on test data = 3000.313295131763\n",
      "R2 on test data = 0.5914366154999713\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol='TotalPrice')\n",
    "dtparamGrid = (ParamGridBuilder()\n",
    "             #.addGrid(dt.maxDepth, [2, 5, 10, 20, 30])\n",
    "             .addGrid(dt.maxDepth, [2, 5, 10])\n",
    "             #.addGrid(dt.maxBins, [10, 20, 40, 80, 100])\n",
    "             .addGrid(dt.maxBins, [10, 20])\n",
    "             .build())\n",
    "dtevaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "dtcv = CrossValidator(estimator=dt,\n",
    "                          estimatorParamMaps=dtparamGrid,\n",
    "                          evaluator=dtevaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "dtcvModel = dtcv.fit(train)\n",
    "dtpredictions = dtcvModel.transform(test)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluate(dtpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 16503.784329678052\n",
      "Mean Squared Error (MSE) on test data = 272374897.20052683\n",
      "MAE on test data = 4397.349797944288\n",
      "R2 on test data = 0.5462499997812253\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(labelCol='TotalPrice')\n",
    "gbtparamGrid = (ParamGridBuilder()\n",
    "             #.addGrid(gbt.maxDepth, [2, 5, 10, 20, 30])\n",
    "             #.addGrid(gbt.maxDepth, [2, 5, 10])\n",
    "                .addGrid(gbt.maxDepth, [2])\n",
    "             #.addGrid(gbt.maxBins, [10, 20, 40, 80, 100])\n",
    "             #.addGrid(gbt.maxBins, [10, 20])\n",
    "                .addGrid(gbt.maxBins, [10])\n",
    "             .build())\n",
    "gbtevaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "gbtcv = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=gbtparamGrid,\n",
    "                          evaluator=gbtevaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "gbtcvModel = gbtcv.fit(train)\n",
    "gbtpredictions = gbtcvModel.transform(test)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluate(gbtpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gbtcvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbtpredictions = best_model.transform(train)#gbtcvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4025.994853766512\n",
      "Mean Squared Error (MSE) on test data = 16208634.562554438\n",
      "MAE on test data = 2415.1828875461715\n",
      "R2 on test data = 0.936428018516429\n"
     ]
    }
   ],
   "source": [
    "evaluate(best_gbtpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 15523.706758546516\n",
      "Mean Squared Error (MSE) on test data = 240985471.52534276\n",
      "MAE on test data = 2939.6750230249017\n",
      "R2 on test data = 0.5985417199557761\n"
     ]
    }
   ],
   "source": [
    "evaluate(gbtpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='TotalPrice')\n",
    "rfparamGrid = (ParamGridBuilder()\n",
    "             #.addGrid(rf.maxDepth, [2, 5, 10, 20, 30])\n",
    "             .addGrid(rf.maxDepth, [2, 5, 10])\n",
    "             #.addGrid(rf.maxBins, [10, 20, 40, 80, 100])\n",
    "             .addGrid(rf.maxBins, [5, 10, 20])\n",
    "             #.addGrid(rf.numTrees, [5, 20, 50, 100, 500])\n",
    "             .addGrid(rf.numTrees, [5, 20, 50])\n",
    "             .build())\n",
    "rfevaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rfcv = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=rfparamGrid,\n",
    "                          evaluator=rfevaluator,\n",
    "                          numFolds=3,\n",
    "                          parallelism=10\n",
    "                     )\n",
    "start = datetime.now()\n",
    "rfcvModel = rfcv.fit(train)\n",
    "print(f'fitting: {datetime.now() - start}')\n",
    "start = datetime.now()\n",
    "rfpredictions = rfcvModel.transform(test)\n",
    "print(f'predicting: {datetime.now() - start}')\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluate(rfpredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfparamGrid = (ParamGridBuilder()\n",
    "             #.addGrid(rf.maxDepth, [2, 5, 10, 20, 30])\n",
    "             .addGrid(rf.maxDepth, [2, 5, 10])\n",
    "             #.addGrid(rf.maxBins, [10, 20, 40, 80, 100])\n",
    "             .addGrid(rf.maxBins, [5, 10, 20])\n",
    "             #.addGrid(rf.numTrees, [5, 20, 50, 100, 500])\n",
    "             .addGrid(rf.numTrees, [5, 20, 50])\n",
    "             .build())\n",
    "rfevaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rfcv = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=rfparamGrid,\n",
    "                          evaluator=rfevaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "Root Mean Squared Error (RMSE) on test data = 14398.532319895226\n",
    "Mean Squared Error (MSE) on test data = 207317732.96706742\n",
    "MAE on test data = 2788.622927146411\n",
    "R2 on test data = 0.6546288870743233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 14398.532319895226\n",
      "Mean Squared Error (MSE) on test data = 207317732.96706742\n",
      "MAE on test data = 2788.622927146411\n",
      "R2 on test data = 0.6546288870743233\n"
     ]
    }
   ],
   "source": [
    "evaluate(rfpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxDepth=30,maxBins=200\n",
      "fitting: 0:00:25.708209\n",
      "predicting: 0:00:00.022701\n",
      "Root Mean Squared Error (RMSE) on test data = 14798.19619096638\n",
      "Mean Squared Error (MSE) on test data = 218986610.5063319\n",
      "MAE on test data = 3215.767711410018\n",
      "R2 on test data = 0.7293016980126208\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "for md in [30]:\n",
    "    for mb in [200]:\n",
    "        print(f'maxDepth={md},maxBins={mb}')\n",
    "        gbt = GBTRegressor(labelCol='TotalPrice',maxDepth=md,maxBins=mb)\n",
    "        start = datetime.now()\n",
    "        gbtModel = gbt.fit(train)\n",
    "        print(f'fitting: {datetime.now() - start}')\n",
    "        start = datetime.now()\n",
    "        prediction = gbtModel.transform(test)\n",
    "        print(f'predicting: {datetime.now() - start}')\n",
    "\n",
    "        #print(gbtModel.featureImportances)\n",
    "        #evaluate(gbtModel.transform(train))\n",
    "        evaluate(gbtModel.transform(test))\n",
    "#maxDepth=30,maxBins=50 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxDepth=30,maxBins=540,numTrees=25\n",
      "fitting: 0:00:06.783697\n",
      "predicting: 0:00:00.031755\n",
      "(7,[0,1,2,3,4,5,6],[0.23178971172965304,0.6769539587106376,0.020128899062635965,0.012058116906790867,0.04466457579549625,0.014242206035673768,0.00016253175911252798])\n",
      "Root Mean Squared Error (RMSE) on test data = 18108.17150612983\n",
      "Mean Squared Error (MSE) on test data = 327905875.2954123\n",
      "MAE on test data = 4576.887264857051\n",
      "R2 on test data = 0.5946621419048501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "for md in [30]:\n",
    "    for mb in [540]:\n",
    "        for nt in [25]:\n",
    "            print(f'maxDepth={md},maxBins={mb},numTrees={nt}')\n",
    "            rf = RandomForestRegressor(labelCol='TotalPrice',maxDepth=md,maxBins=mb,numTrees=nt)\n",
    "            start = datetime.now()\n",
    "            rfModel = rf.fit(train)\n",
    "            print(f'fitting: {datetime.now() - start}')\n",
    "            start = datetime.now()\n",
    "            prediction = rfModel.transform(test)\n",
    "            print(f'predicting: {datetime.now() - start}')\n",
    "\n",
    "            print(rfModel.featureImportances)\n",
    "            #evaluate(rfModel.transform(train))\n",
    "            evaluate(rfModel.transform(test))\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 19197.50159968777\n",
      "Mean Squared Error (MSE) on test data = 368544067.67001444\n",
      "MAE on test data = 7856.933766853909\n",
      "R2 on test data = 0.31343925428569874\n"
     ]
    }
   ],
   "source": [
    "GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2821.6695211504534\n",
      "Mean Squared Error (MSE) on test data = 7961818.886589429\n",
      "MAE on test data = 1035.8291532099954\n",
      "R2 on test data = 0.9564494735427892\n"
     ]
    }
   ],
   "source": [
    "evaluate(rfModel.transform(train))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(2,[0,1],[0.471115392692647,0.5288846073073529])\n",
    "Root Mean Squared Error (RMSE) on test data = 14580.387657256299\n",
    "Mean Squared Error (MSE) on test data = 212587704.23587182\n",
    "MAE on test data = 2958.868961149455\n",
    "R2 on test data = 0.6458496291876744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxIter=5,regParam=1,elasticNetParam=0.5\n",
      "fitting: 0:00:20.168227\n",
      "predicting: 0:00:00.019501\n",
      "Root Mean Squared Error (RMSE) on test data = 2534.4585315564923\n",
      "Mean Squared Error (MSE) on test data = 6423480.048179491\n",
      "MAE on test data = 1383.9785043579686\n",
      "R2 on test data = 0.9920596737039239\n",
      "\n",
      "maxIter=5,regParam=1,elasticNetParam=1\n",
      "fitting: 0:00:08.235003\n",
      "predicting: 0:00:00.017723\n",
      "Root Mean Squared Error (RMSE) on test data = 2533.7734799090654\n",
      "Mean Squared Error (MSE) on test data = 6420008.047490494\n",
      "MAE on test data = 1383.7326368550755\n",
      "R2 on test data = 0.99206396558592\n",
      "\n",
      "maxIter=10,regParam=1,elasticNetParam=0.5\n",
      "fitting: 0:00:06.798363\n",
      "predicting: 0:00:00.014347\n",
      "Root Mean Squared Error (RMSE) on test data = 558.3785398931979\n",
      "Mean Squared Error (MSE) on test data = 311786.5938132597\n",
      "MAE on test data = 379.82250520501606\n",
      "R2 on test data = 0.999614587844743\n",
      "\n",
      "maxIter=10,regParam=1,elasticNetParam=1\n",
      "fitting: 0:00:06.592279\n",
      "predicting: 0:00:00.014787\n",
      "Root Mean Squared Error (RMSE) on test data = 557.7863605753024\n",
      "Mean Squared Error (MSE) on test data = 311125.6240438411\n",
      "MAE on test data = 379.2547089774217\n",
      "R2 on test data = 0.9996154048964971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "for mi in [5,10]:\n",
    "    for rp in [1]:\n",
    "        for en in [0.5, 1]:\n",
    "            print(f'maxIter={mi},regParam={rp},elasticNetParam={en}')\n",
    "            lr = LinearRegression(labelCol='TotalPrice', maxIter=mi, regParam=rp, elasticNetParam=en)\n",
    "            start = datetime.now()\n",
    "            lrModel = lr.fit(train)\n",
    "            print(f'fitting: {datetime.now() - start}')\n",
    "            start = datetime.now()\n",
    "            prediction = lrModel.transform(test)\n",
    "            print(f'predicting: {datetime.now() - start}')\n",
    "\n",
    "            #print(lrModel.featureImportances)\n",
    "            #evaluate(lrModel.transform(train))\n",
    "            evaluate(lrModel.transform(test))\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.04654540268024453,0.9826257680094383,3.0871702069784193,-0.5985103700892468,-244.39200825273446,0.09882774813417344,443.65606303735547]\n",
      "Intercept: 501.6111374428941\n",
      "numIterations: 11\n",
      "objectiveHistory: [0.5000000000000002, 0.36030420263345214, 0.1135579263279735, 0.04392768755876902, 0.01911365596350906, 0.01605146499418765, 0.011711971940388342, 0.004945097905512382, 0.0041361614504214685, 0.0027977211384173175, 0.0002987365135778784]\n",
      "RMSE: 344.765390\n",
      "r2: 0.999441\n",
      "Root Mean Squared Error (RMSE) on test data = 396.9319101074172\n",
      "Mean Squared Error (MSE) on test data = 157554.94126152265\n",
      "MAE on test data = 332.0016169723292\n",
      "R2 on test data = 0.999805239895852\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(labelCol='TotalPrice', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "\n",
    "#trainingSummary.residuals.show()\n",
    "\n",
    "prediction = lrModel.transform(test)\n",
    "evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(agg.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = np.array(agg.collect())[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a[:305,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array(agg.drop('Date').collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array(prediction.select('prediction').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = p[:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(periods), a[:,2])\n",
    "plt.plot(np.sort(periods), pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:5,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}