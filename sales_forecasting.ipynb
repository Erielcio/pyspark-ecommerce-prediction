{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.getConf()#.get(\"spark.executor.instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SparkConf().setAll([\n",
    "    ('spark.executor.cores', '2'), ('spark.executor.memory', '4g'), ('spark.driver.memory','4g'), ('spark.submit.deployMode','client')\n",
    "])\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=config)\n",
    "\n",
    "#sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv('user_ecommerce-data.csv', header=True, inferSchema=True)\n",
    "df = df.drop('CustomerID', 'UnitPrice')\n",
    "df = df.withColumn(\"Date\", to_date(col(\"Date\")))\n",
    "\n",
    "#df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(df, column):\n",
    "    quantiles = df.stat.approxQuantile(column, [0.05, 0.95], 0.0)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lowerRange = Q1 - 1.5 * IQR\n",
    "    upperRange = Q3 + 1.5 * IQR\n",
    "    df.filter((df[column] < lowerRange) | (df[column] > upperRange)).show()\n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    quantiles = df.stat.approxQuantile(column, [0.05, 0.95], 0.0)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lowerRange = Q1 - 1.5 * IQR\n",
    "    upperRange = Q3 + 1.5 * IQR\n",
    "    df = df.filter((df[column] >= lowerRange) | (df[column] <= upperRange))\n",
    "\n",
    "remove_outliers(df, 'Quantity')\n",
    "remove_outliers(df, 'TotalPrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = df.select('Date', 'Quantity', 'TotalPrice').groupby('Date').sum()\n",
    "agg = agg.withColumnRenamed('sum(Quantity)', 'Quantity')\n",
    "agg = agg.withColumnRenamed('sum(TotalPrice)', 'TotalPrice')\n",
    "\n",
    "agg = agg.withColumn('Month', month('Date'))\n",
    "agg = agg.withColumn('DayOfMonth', dayofmonth('Date'))\n",
    "agg = agg.withColumn('DayOfWeek', dayofweek('Date'))\n",
    "agg = agg.withColumn('DayOfYear', dayofyear('Date'))\n",
    "agg = agg.withColumn('Weekend', dayofweek('Date').isin(6,7).cast('int'))\n",
    "\n",
    "#agg.show()\n",
    "agg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "agg = agg.orderBy(asc(\"Date\"))\n",
    "pd_agg = agg.toPandas()\n",
    "pd_agg['Date'].count() * 0.9, pd_agg['Date'].count() * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "275+30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "agg = agg.orderBy(asc(\"Date\"))\n",
    "agg_train = agg.limit(275)\n",
    "agg_test = agg.orderBy(desc(\"Date\")).limit(30).orderBy(\"Date\")\n",
    "#train.show()\n",
    "#test.show()\n",
    "\n",
    "#pd_train = agg_train.toPandas()\n",
    "#pd_test = agg_test.toPandas()\n",
    "#pd_train['Date'].unique(), pd_test['Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bucketizer = Bucketizer(splits=[ 0, 2, 5, 8, 11, 14, 15, 5000], inputCol=\"Quantity\", outputCol=\"QuantityRange\")\n",
    "df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "\n",
    "bucketizer = Bucketizer(splits=[ 0, 1, 2, 3, 4, 20], inputCol=\"UnitPrice\", outputCol=\"PriceRange\")\n",
    "df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"Month\", outputCol=\"DateRange\")\n",
    "result = discretizer.fit(df).transform(df)\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "si = StringIndexer(inputCol='StockCode', outputCol='StockCodeIndex')\n",
    "df = si.fit(df).transform(df)\n",
    "\n",
    "si = StringIndexer(inputCol='Country', outputCol='CountryIndex')\n",
    "df = si.fit(df).transform(df)\n",
    "\n",
    "df = df.withColumn(\"CountryIndex\", df[\"CountryIndex\"].cast('int'))\n",
    "df = df.withColumn(\"StockCodeIndex\", df[\"StockCodeIndex\"].cast('int'))\n",
    "\n",
    "df = df.withColumn(\"QuantityRange\", df[\"QuantityRange\"].cast('int'))\n",
    "df = df.withColumn(\"PriceRange\", df[\"PriceRange\"].cast('int'))\n",
    "\n",
    "df = df.withColumn(\"Cluster\", df[\"Cluster\"].cast('int'))\n",
    "#df.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    #inputCols=['Quantity', 'UnitPrice', 'QuantityRange', 'PriceRange', 'Month', 'CountryIndex', 'StockCodeIndex', 'Cluster'],\n",
    "    inputCols=['Quantity','TotalPrice','Month','DayOfMonth','DayOfWeek','DayOfYear','Weekend'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "train = assembler.transform(agg_train).select(['features', 'TotalPrice'])\n",
    "test = assembler.transform(agg_test).select(['features', 'TotalPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prediction):\n",
    "    evaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    print(f\"Root Mean Squared Error (RMSE) on test data = {evaluator.evaluate(prediction)}\")\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "    print(f\"Mean Squared Error (MSE) on test data = {evaluator.evaluate(prediction)}\")\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    print(f\"MAE on test data = {evaluator.evaluate(prediction)}\")\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    print(f\"R2 on test data = {evaluator.evaluate(prediction)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#lr = LinearRegression(maxIter=5, regParam=0.3, elasticNetParam=0.8)\n",
    "lr = LinearRegression(labelCol='TotalPrice')\n",
    "lrparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.001, 0.01, 0.1, 0.5, 1.0, 2.0])\n",
    "             #.addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "             #.addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10, 20, 50])\n",
    "             #.addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "lrevaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "lrcv = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=lrparamGrid,\n",
    "                          evaluator=lrevaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "lrcvModel = lrcv.fit(train)\n",
    "\n",
    "#lrcvSummary = lrcvModel.bestModel.summary\n",
    "#print(\"Coefficient Standard Errors: \" + str(lrcvSummary.coefficientStandardErrors))\n",
    "#print(\"P Values: \" + str(lrcvSummary.pValues)) # Last element is the intercept\n",
    "\n",
    "lrpredictions = lrcvModel.transform(test)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluate(lrpredictions)\n",
    "\n",
    "#import numpy as np\n",
    "#lrcvModel.getEstimatorParamMaps()[ np.argmax(lrcvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol='TotalPrice')\n",
    "dtparamGrid = (ParamGridBuilder()\n",
    "             #.addGrid(dt.maxDepth, [2, 5, 10, 20, 30])\n",
    "             .addGrid(dt.maxDepth, [2, 5, 10])\n",
    "             #.addGrid(dt.maxBins, [10, 20, 40, 80, 100])\n",
    "             .addGrid(dt.maxBins, [10, 20])\n",
    "             .build())\n",
    "dtevaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "dtcv = CrossValidator(estimator=dt,\n",
    "                          estimatorParamMaps=dtparamGrid,\n",
    "                          evaluator=dtevaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "dtcvModel = dtcv.fit(train)\n",
    "dtpredictions = dtcvModel.transform(test)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluate(dtpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(labelCol='TotalPrice')\n",
    "gbtparamGrid = (ParamGridBuilder()\n",
    "             #.addGrid(gbt.maxDepth, [2, 5, 10, 20, 30])\n",
    "             #.addGrid(gbt.maxDepth, [2, 5, 10])\n",
    "                .addGrid(gbt.maxDepth, [2])\n",
    "             #.addGrid(gbt.maxBins, [10, 20, 40, 80, 100])\n",
    "             #.addGrid(gbt.maxBins, [10, 20])\n",
    "                .addGrid(gbt.maxBins, [10])\n",
    "             .build())\n",
    "gbtevaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "gbtcv = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=gbtparamGrid,\n",
    "                          evaluator=gbtevaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "gbtcvModel = gbtcv.fit(train)\n",
    "gbtpredictions = gbtcvModel.transform(test)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluate(gbtpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='TotalPrice')\n",
    "rfparamGrid = (ParamGridBuilder()\n",
    "             #.addGrid(rf.maxDepth, [2, 5, 10, 20, 30])\n",
    "             .addGrid(rf.maxDepth, [2, 5, 10])\n",
    "             #.addGrid(rf.maxBins, [10, 20, 40, 80, 100])\n",
    "             .addGrid(rf.maxBins, [5, 10, 20])\n",
    "             #.addGrid(rf.numTrees, [5, 20, 50, 100, 500])\n",
    "             .addGrid(rf.numTrees, [5, 20, 50])\n",
    "             .build())\n",
    "rfevaluator = RegressionEvaluator(labelCol=\"TotalPrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rfcv = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=rfparamGrid,\n",
    "                          evaluator=rfevaluator,\n",
    "                          numFolds=3,\n",
    "                          parallelism=10\n",
    "                     )\n",
    "start = datetime.now()\n",
    "rfcvModel = rfcv.fit(train)\n",
    "print(f'fitting: {datetime.now() - start}')\n",
    "start = datetime.now()\n",
    "rfpredictions = rfcvModel.transform(test)\n",
    "print(f'predicting: {datetime.now() - start}')\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluate(rfpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "for md in [30]:\n",
    "    for mb in [200]:\n",
    "        print(f'maxDepth={md},maxBins={mb}')\n",
    "        gbt = GBTRegressor(labelCol='TotalPrice',maxDepth=md,maxBins=mb)\n",
    "        start = datetime.now()\n",
    "        gbtModel = gbt.fit(train)\n",
    "        print(f'fitting: {datetime.now() - start}')\n",
    "        start = datetime.now()\n",
    "        prediction = gbtModel.transform(test)\n",
    "        print(f'predicting: {datetime.now() - start}')\n",
    "\n",
    "        #print(gbtModel.featureImportances)\n",
    "        #evaluate(gbtModel.transform(train))\n",
    "        evaluate(gbtModel.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "for md in [30]:\n",
    "    for mb in [540]:\n",
    "        for nt in [25]:\n",
    "            print(f'maxDepth={md},maxBins={mb},numTrees={nt}')\n",
    "            rf = RandomForestRegressor(labelCol='TotalPrice',maxDepth=md,maxBins=mb,numTrees=nt)\n",
    "            start = datetime.now()\n",
    "            rfModel = rf.fit(train)\n",
    "            print(f'fitting: {datetime.now() - start}')\n",
    "            start = datetime.now()\n",
    "            prediction = rfModel.transform(test)\n",
    "            print(f'predicting: {datetime.now() - start}')\n",
    "\n",
    "            print(rfModel.featureImportances)\n",
    "            #evaluate(rfModel.transform(train))\n",
    "            evaluate(rfModel.transform(test))\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "for mi in [5,10]:\n",
    "    for rp in [1]:\n",
    "        for en in [0.5, 1]:\n",
    "            print(f'maxIter={mi},regParam={rp},elasticNetParam={en}')\n",
    "            lr = LinearRegression(labelCol='TotalPrice', maxIter=mi, regParam=rp, elasticNetParam=en)\n",
    "            start = datetime.now()\n",
    "            lrModel = lr.fit(train)\n",
    "            print(f'fitting: {datetime.now() - start}')\n",
    "            start = datetime.now()\n",
    "            prediction = lrModel.transform(test)\n",
    "            print(f'predicting: {datetime.now() - start}')\n",
    "\n",
    "            #print(lrModel.featureImportances)\n",
    "            #evaluate(lrModel.transform(train))\n",
    "            evaluate(lrModel.transform(test))\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(labelCol='TotalPrice', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "\n",
    "#trainingSummary.residuals.show()\n",
    "\n",
    "prediction = lrModel.transform(test)\n",
    "evaluate(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
