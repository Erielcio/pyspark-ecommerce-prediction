{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SparkConf().setAll([\n",
    "    ('spark.executor.cores', '2'), ('spark.executor.memory', '4g'), ('spark.driver.memory','4g'), ('spark.submit.deployMode','client')\n",
    "])\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=config)\n",
    "\n",
    "#sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv('ecommerce-data.csv', header=True)\n",
    "\n",
    "df = df.na.drop()\n",
    "df = df.withColumn(\"CustomerID\", df[\"CustomerID\"].cast('int'))\n",
    "df = df.withColumn(\"Quantity\", df[\"Quantity\"].cast('int'))\n",
    "df = df.withColumn(\"UnitPrice\", df[\"UnitPrice\"].cast('float'))\n",
    "\n",
    "df = df.withColumn(\"Date\", to_date(col(\"InvoiceDate\"),\"MM/dd/yyyy\"))\n",
    "\n",
    "df = df.filter((df['Quantity'] > 0) & (df['UnitPrice'] > 0))\n",
    "\n",
    "df = df.withColumn('TotalPrice', df['Quantity'] * df['UnitPrice'])\n",
    "df.count()\n",
    "\n",
    "#df.show()\n",
    "#df.show(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "now = to_date(lit(dt.date(2011,12,9)))\n",
    "\n",
    "recency_df = df.groupby('CustomerID').agg(max('Date').alias('LastPurchaseDate'))\n",
    "recency_df = recency_df.withColumn(\"Diff\", datediff(now, to_date(recency_df['LastPurchaseDate'])))\n",
    "recency_df = recency_df.groupby('CustomerID').agg(min('Diff').alias('Recency'))\n",
    "\n",
    "frequency_df = df.groupby('CustomerID').agg(count('InvoiceNo').alias('Frequency'))\n",
    "\n",
    "monetary_df = df.groupby('CustomerID').agg(sum('TotalPrice').alias('Monetary'))\n",
    "\n",
    "rfm = recency_df.join(frequency_df, on=['CustomerID'], how='inner')\n",
    "rfm = rfm.join(monetary_df, on=['CustomerID'], how='inner')\n",
    "#rfm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(rfm, column):\n",
    "    quantiles = rfm.stat.approxQuantile(column, [0.05, 0.95], 0.0)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lowerRange = Q1 - 1.5 * IQR\n",
    "    upperRange = Q3 + 1.5 * IQR\n",
    "    rfm.filter((rfm[column] < lowerRange) | (rfm[column] > upperRange)).show()\n",
    "\n",
    "def remove_outliers(rfm, column):\n",
    "    quantiles = rfm.stat.approxQuantile(column, [0.05, 0.95], 0.0)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lowerRange = Q1 - 1.5 * IQR\n",
    "    upperRange = Q3 + 1.5 * IQR\n",
    "    rfm = rfm.filter((rfm[column] >= lowerRange) | (rfm[column] <= upperRange))\n",
    "\n",
    "#get_outliers(rfm, 'Recency')\n",
    "#get_outliers(rfm, 'Frequency')\n",
    "#get_outliers(rfm, 'Monetary')\n",
    "remove_outliers(rfm, 'Recency')\n",
    "remove_outliers(rfm, 'Frequency')\n",
    "remove_outliers(rfm, 'Monetary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Recency', 'Frequency', 'Monetary'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "rfm_feat = rfm.withColumnRenamed('CustomerID', 'id')\n",
    "\n",
    "rfm_feat = assembler.transform(rfm_feat).select('id', 'features')\n",
    "#rfm_feat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(rfm_feat)\n",
    "\n",
    "rfm_final = scalerModel.transform(rfm_feat).select('id', 'scaledFeatures')\n",
    "rfm_final = rfm_final.withColumnRenamed('scaledFeatures', 'features')\n",
    "#rfm_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_of_partition(map_of_rows):\n",
    "    list_of_rows = list(map_of_rows)\n",
    "    size_of_list = len(list_of_rows)\n",
    "    return [size_of_list]\n",
    "\n",
    "#df.rdd.mapPartitions(size_of_partition).collect()\n",
    "#rfm_final.rdd.getNumPartitions()\n",
    "#reparted_rdd = df.rdd.repartition(20)\n",
    "#reparted_rdd.mapPartitions(size_of_partition).collect()\n",
    "#reparted_rdd.map(lambda x: x).toDF().show(reparted_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "n_clusters = 10\n",
    "cost = np.zeros(n_clusters)\n",
    "silh_val = []\n",
    "silh_lst = []\n",
    "\n",
    "for k in range(2, n_clusters):\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setMaxIter(50).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(rfm_final)\n",
    "    \n",
    "    # elbow method\n",
    "    cost[k] = model.summary.trainingCost\n",
    "        \n",
    "    predictions = model.transform(rfm_final)\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silh_val.append(silhouette)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbs\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize =(10,5))\n",
    "ax.plot(range(2, n_clusters),cost[2:n_clusters])\n",
    "plt.xlabel('K - Clusters')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow Curve')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silh_array = np.asanyarray(silh_val)\n",
    "silhouette = pd.DataFrame(list(zip(range(2, n_clusters),silh_array)),columns = ['K - Clusters', 'silhouette'])\n",
    "silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "k = 3\n",
    "kmeans = KMeans().setK(k).setSeed(1).setMaxIter(50).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(rfm_final)\n",
    "predictions = model.transform(rfm_final)\n",
    "predictions = predictions.withColumnRenamed('id', 'CustomerID')\n",
    "predictions = predictions.withColumnRenamed('prediction', 'Cluster')\n",
    "predictions = predictions.select(['CustomerID', 'Cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary.clusterSizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = df.join(predictions, on=['CustomerID'], how='inner')\n",
    "#df_cluster.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.write.csv('user_ecommerce-data.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
